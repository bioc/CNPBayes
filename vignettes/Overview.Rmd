---
title: "Overview of CNPBayes package"
author: "Jacob Carey, Steven Cristiano, and Robert Scharpf"
date: \today
output: BiocStyle::pdf_document
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{Overview of CNPBayes package}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc} 
---

# Introduction

CNPBayes models multi-modal densities via a hierarchical Bayesian Gaussian mixture model. The major application of this model is the estimation of copy number at copy number polymorphic loci (CNPs). Four versions of the mixture model are implemented: a *standard* model, referred to as a *SingleBatch* (SB) model , that has one mean and standard deviation for each component; a *SingleBatchPooled* (SBP) model that has a pooled estimate of the standard deviation across all mixture components; a *MultiBatch* (MB) model with batch-specific means and standard deviations; and a *MultiBatchPooled* (MBP) model with batch-specific standard deviations that are pooled across mixture components within each batch. For all versions, approximation of the posterior is by Markov Chain Monte Carlo (MCMC) written in C++ using the Rcpp package [@Rcpp].

For an EM-implementation of Gaussian mixture models for CNPs, see the Bioconductor package CNVtools [@Barnes2008]. A Bayesian extension of this model by some of the same authors was developed to automate the analysis of the Welcome Trust Case Control Consortium (WTCCC) genotype data [@cardin] and implemented in the R package CNVCALL (http://niallcardin.com/CNVCALL).

This vignette provides a concise workfklow for fitting mixture models in large array-based genome-wide association studies. Other vignettes in this package provide details on the [implementation](Implementation.Rmd) and functions for evaluating [convergence](Convergence.Rmd).

```{r lib} 
suppressPackageStartupMessages(library(CNPBayes))
suppressPackageStartupMessages(library(SummarizedExperiment))
```

# Workflow

## Delineate CNPs for each ancestry group


Our starting point for the evaluation of copy number at polymorphic loci is a collection of copy number variant intervals for a large number of samples provided as a `GRangesList` and summary statistics of the median log R ratio for each sample in a CNP provided as a `SummarizedExperiment`.  The `CNPBayes` package provides an example of a  `SnpArrayExperiment` and  a `GRangesList` object for this purpose.  Specifically, the `GRangesList` object contains a collection of copy number deletions and duplications identified from a hidden Markov model fit independently to each sample in a study.   The `SummarizedExperiment` object contains log R ratios and, optionally, B Allele frequencies, obtained from a SNP array.

```{r find_cnps}
se <- readRDS(system.file("extdata", "simulated_se.rds", package="CNPBayes"))
grl <- readRDS(system.file("extdata", "grl_deletions.rds", package="CNPBayes"))
```


Using this data, we identify CNP loci and summarize within sample and locus using the median. We advise against using the first principal component as a summary statistic in large studies. See [Identifying Copy Number Polymorphisms](FindCNPs.pdf) for instructions on finding CNPs with a `SnpArrayExperiment` and `GRangesList`.

```{r summary, message=FALSE}
cnv.region <- consensusCNP(grl, max.width=5e6)
i <- subjectHits(findOverlaps(cnv.region, rowRanges(se)))
med.summary <- matrixStats::colMedians(assays(se)[["cn"]][i, ], na.rm=TRUE)
```

We recommend using the `collapseBatch` function to identify groups of samples  that belong to the same batch. However, as there are only 35 samples in this toy example, we assume that these samples were all processed in the same batch. In the following code chunk, we initialize an MCMC parameters object that contain parameters governing the number of burnin iterations, the number of iterations following burnin, and the number of independently initialized models (*nStarts*).

```{r model_construction}
mp <- McmcParams(nStarts=5, burnin=100, iter=1000)
```

The workhorse function in this package is `gibbs`.  This function allows the user to specify one or all of the four possible mixture model types and for each type fit models with mixture components specified by the argument `k_range`. Below, we run 4 chains for 1000 iterations (100 iterations burnin) for only the k=3 SB model. (In practice, we would fit multiple models and specify a range for k -- e.g, `k_range=c(1, 5)`).  As described in the [convergence vignette](Convergence.Rmd), an attempt is made by `gibbs` to adapt the thinning and burnin parameters to attain convergence.  The models evaluated by `gibbs` are returned in a list where all chains have been combined and the models are sorted by decreasing value of the marginal likelihood.  In order to properly assess convergence, this function requires that one run at least 2 independent chains. Below, we specify a `max_burnin` to a small number (400) to speed up computation. A much longer burnin may be required for convergence.

```{r gibbs}
model.list <- gibbs(model="SB", dat=med.summary, k_range=c(2, 4), mp=mp,
                    max_burnin=400)
```

In the following code chunk, we inspect the chains for convergence and overlay the posterior summary of the mixture model on a histogram of the data.

```{r ggfunctions}
model <- model.list[[1]]
chains <- ggChains(model)
chains[[1]]
chains[[2]]
```

For details about model construction and mapping mixture components to copy number states, see [Bayesian mixture models for copy number estimation](Implementation.pdf). 


## Big data

If thousands of samples are available, we generally do not need to fit the model to all samples in order to adequately estimate the mixture distribution. Below, we indicate a workflow for downsampling. First, we bin the data in the `MultiBatchModelExample`, requiring only 200 observations per batch.  Data in small batches (less than 200 samples) are not binned.  Next, we use the `tileSummaries` function to compute the average log R ratio for each bin.

```{r downsample}
mb <- MultiBatchModelExample
tiled.medians <- tileMedians(y(mb), 200, batch(mb))
tile.summaries <- tileSummaries(tiled.medians)
tile.summaries
```

Next, we fit the model to the mean summarized bins in the usual way. We specify the true model in this toy example to speed up computation.


```{r fit_downsample}
## too long
model <- gibbs(model="MB", k_range=c(3, 3),
               dat=tile.summaries$avgLRR,
               batches=tile.summaries$batch,
               mp=mp)[[1]]
```

To map posterior probabilities back to the original observations, we use the function `upSample`.

```{r upSample}
model2 <- upSample(model, tiled.medians)
```

As described in the [implementation vignetted](Implementation.Rmd), the mapping of mixture components to copy number states may not be one-to-one.  Here, we convert the mixture model to a copy number model that retains a one-to-one mapping and compute the posterior probabilities for the copy number states.

```{r copy-number-model}
cn.model <- CopyNumberModel(model2)
mapping(cn.model)
round(head(probz(cn.model)), 2)
```

See the [convergence vignette](Convergence.Rmd) for assessing convergence and visually inspecting the MCMC chains for each parameter in the mixture model.

# References
