---
title: "Identifying Copy Number Polymorphisms"
author: "Jacob Carey, Steven Cristiano, and Robert Scharpf"
date: \today
output: BiocStyle::pdf_document
vignette: >
  %\VignetteIndexEntry{Identifying Copy Number Polymorphisms}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc} 
---

# Introduction

Identify consensus start and stop coordinates of a copy number
polymorphism

The collection of copy number variants (CNVs) identified in a study
can be encapulated in a GRangesList, where each element is a GRanges
of the CNVs identified for an individual.  (For a study with 1000
subjects, the GRangesList object would have length 1000 if each
individual had 1 or more CNVs.)  For regions in which CNVs occur in
more than 2 percent of study participants, the start and end
boundaries of the CNVs may differ because of biological differences in
the CNV size as well as due to technical noise of the assay and the
uncertainty of the breakpoints identified by a segmentation of the
genomic data.  Among subjects with a CNV called at a given locus, the
`consensusCNP` function identifies the largest region that is copy
number variant in half of these subjects.

# Finding CNPs
Included in the CNPBayes package are objects of class `SnpArrayExperiment`
and `GRangesList`. We begin by loading the necessary libraries and data.

```{r prelim}
knitr::opts_chunk$set(eval=FALSE)
library(CNPBayes)
library(VanillaICE)
se <- readRDS(system.file("extdata", "simulated_se.rds", package="CNPBayes"))
grl <- readRDS(system.file("extdata", "grl_deletions.rds", package="CNPBayes"))
```

The object `se` contains log R ratios and B Allele Frequencies, and the object
`grl` is a `GRangesList` of simulated deletions. 

After reading this saved data, we visualize the CNVs.

```{r plot-cnvs}
xlim <- c(min(start(se)), max(end(se)))
xlim <- c(10e6, 40e6)
par(las=1)
plot(0, xlim=xlim, ylim=c(0, 26), xlab="Mb", ylab="sample index", type="n",
     xaxt="n")
at <- pretty(xlim, n=10)
axis(1, at=at, labels=round(at/1e6, 1), cex.axis=0.8)
rect(start(se), seq_along(se)-0.2, end(se), seq_along(se)+0.2,
     col="gray", border="gray")
```

Before a `MixtureModel` can be fit, the log R ratios must be summarized. One
method for summary is to use the first principal component to summarize the log
R ratios. A possible disadvantage of this approach is that the scale of the
loadings makes it more difficult to interpret the copy number of the mixture
components. Instead, the median log R ratio is adequate and retains the
original scale.

To summarize samples by the median log R ratios, we define the largest region
that spans 50 percent of the samples using the function `consensusCNP`. Because
the deletions in this example are large (great than 2 Mb), we specify a large
value for `max.width` to avoid filter ing these CNVs.

# Median

# PCA

```{r summary}
# median
cnv.region <- consensusCNP(grl, max.width=5e6)
i <- subjectHits(findOverlaps(cnv.regions, se))
med.summary <- matrixStats::colMedians(lrr(se)[i, ], na.rm=TRUE)

# PCA
cnv.region2 <- reduce(grl)
i.pc <- subjectHits(findOverlaps(cnv.region2, se))
x <- lrr(se)[i.pc, ]
nas <- rowSums(is.na(x))
na.index <- which(nas > 0)
x <- x[-na.index, , drop=FALSE]
pc.summary <- prcomp(t(x))$x[, 1]
meds.for.pc <- matrixStats::colMedians(x, na.rm=TRUE)
if(cor(pc.summary, meds.for.pc) < 1) pc.summary <- -1*pc.summary
```

Finally we plot the one dimensional summaries.

```{r summary-plots}
par(mfrow=c(1,2), las=1)
plot(med.summary, main="median summary of\nconsensus CNP", cex.main=0.7, pch=20)
plot(pc.summary, main="PC summary of\nreduced CNV ranges", cex.main=0.7, pch=20)
```

Please refer to the CNPBayes vignette for fitting a `MixtureModel`.
